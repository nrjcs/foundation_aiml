{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rnn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nrjcs/foundation_aiml/blob/master/rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tn1Vf9BRoxnU",
        "colab_type": "text"
      },
      "source": [
        "This is RNN example based on the example from https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/6.2-understanding-recurrent-neural-networks.ipynb\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6teOUcJ-kssi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2be82246-13b9-4c9f-d789-170006c1bbf9"
      },
      "source": [
        "from keras.models import Sequential # importing Sequential model from keras models\n",
        "from keras.layers import Embedding, SimpleRNN # importing Embedding layer and SimpleRNN layer from keras layers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-W46iYQa_yGT",
        "colab_type": "text"
      },
      "source": [
        "# Embedding Layer\n",
        "\n",
        "> details @ https://keras.io/layers/embeddings/\n",
        "\n",
        "> used to create an embedding....embed higher dimensional data into lower dimensional vector space\n",
        "\n",
        "> Embedding layer is the first layer in the model\n",
        "\n",
        "> Keras provides method to add an embedding layer\n",
        "\n",
        "> keras.layers.Embedding(input_dim, output_dim, embeddings_initializer='uniform', .... , input_length=None)\n",
        "\n",
        "> * input_dim: An interger greater than 0 which specifies the size of the vocabulary, i.e. maximum integer index + 1 such that data is integer encoded from 0 - maximum integer index. It can be thought of as the number of distinct words.\n",
        "\n",
        "> * output_dim: An integer greater than 0 which specifies the dimension of vector space in which words will be embedded\n",
        "\n",
        "> * embeddings_initializer: Initializer for the embeddings matrix...Keras provides support to many initializers such as uniform, RandomNormal, RandomUniform, and so on (details @ https://keras.io/initializers/) \n",
        "\n",
        "\n",
        "> * input_length: Length of input sequences, when it is constant. This argument is required if you are going to connect Flatten then Dense layers upstream (without it, the shape of the dense outputs cannot be computed)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rUy_kYLF7sj",
        "colab_type": "text"
      },
      "source": [
        "# SimpleRNN\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "> to add simple recurrent neural network layers\n",
        "\n",
        "> keras.layers.SimpleRNN(units, activation='tanh', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', return_sequences=False, return_state=False, ......)\n",
        "\n",
        "> * details @ https://keras.io/layers/recurrent/\n",
        "\n",
        "> * creates Fully-connected RNN where the output is to be fed back to input.\n",
        "\n",
        "> * units: Positive integer, dimensionality of the output space.\n",
        "\n",
        "> * activation: Activation function to use (see activations). Default: hyperbolic tangent (tanh). If you pass None, no activation is applied (ie. \"linear\" activation: a(x) = x).\n",
        "\n",
        "> * use_bias: Boolean, whether the layer uses a bias vector.\n",
        "\n",
        "> * kernel_initializer: Initializer for the kernel weights matrix, used for the linear transformation of the inputs\n",
        "\n",
        "> * return_sequences: Boolean. If true,  returns the full sequences of successive outputs for each timestep (a 3D tensor of shape (batch_size, timesteps, output_features)), however, if false returns only the last output for each input sequence (a 2D tensor of shape (batch_size, output_features))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvU7IeJcNsQP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "!pip install numpy==1.16.1\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eleKvl3iMOUi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# using a RNN model on the IMDB movie review classification problem\n",
        "\n",
        "# data  preprocessing\n",
        "\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "max_features = 10000  # number of words to consider as features\n",
        "maxlen = 500  # cut texts after this number of words (among top max_features most common words)\n",
        "batch_size = 32\n",
        "\n",
        "print('Loading data...')\n",
        "(input_train, y_train), (input_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "print(len(input_train), 'train sequences')\n",
        "print(len(input_test), 'test sequences')\n",
        "\n",
        "print('Pad sequences (samples x time)')\n",
        "# pad_sequences is used to ensure that all sequences in a list have the same length. \n",
        "# Keras sequence.pad_sequence(): This function transforms a list of num_samples sequences (lists of integers) into a 2D \n",
        "# Numpy array of shape (num_samples, num_timesteps). num_timesteps is either the maxlen argument if provided, or the\n",
        "# length of the longest sequence otherwise.\n",
        "input_train = sequence.pad_sequences(input_train, maxlen=maxlen)\n",
        "input_test = sequence.pad_sequences(input_test, maxlen=maxlen)\n",
        "print('input_train shape:', input_train.shape)\n",
        "print('input_test shape:', input_test.shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5EYei_0LxwS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define a simple recurrent network using an Embedding layer and a SimpleRNN layer\n",
        "from keras.layers import Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 32))\n",
        "model.add(SimpleRNN(32))\n",
        "model.add(SimpleRNN(32))\n",
        "model.add(SimpleRNN(32))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "up835lxOM7-M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# configure learning\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PN18I3gvM8vb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train \n",
        "history = model.fit(input_train, y_train, epochs=10, batch_size=128, validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kl78tieNM32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot raining and validation loss and accuracy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mSer0diSpmv",
        "colab_type": "text"
      },
      "source": [
        "**Few Sources to Explore**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        " https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/6.2-understanding-recurrent-neural-networks.ipynb\n",
        "\n",
        "https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
        "\n",
        "https://keras.io/layers/embeddings/\n",
        "\n",
        "https://keras.io/initializers/\n",
        "\n"
      ]
    }
  ]
}